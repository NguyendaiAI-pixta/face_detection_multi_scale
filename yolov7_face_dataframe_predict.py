# YOLOv7-Face Detection v·ªõi DataFrame
# Script n√†y predict faces s·ª≠ d·ª•ng YOLOv7-Face model cho d·ªØ li·ªáu t·ª´ DataFrame:
# 
# T√≠nh nƒÉng:
# 1. Load DataFrame v·ªõi columns `item_id` v√† `tiny_face_module` 
# 2. T·ª´ ƒë∆∞·ªùng d·∫´n base `tiny_face_module`, t√¨m t·∫•t c·∫£ ·∫£nh `_original_xxx.jpg`
# 3. Predict faces cho t·∫•t c·∫£ frames c·ªßa m·ªói item_id
# 4. L∆∞u k·∫øt qu·∫£ JSON theo format chu·∫©n cho t·ª´ng item_id
# 5. S·ª≠ d·ª•ng multiprocessing v·ªõi GPU ƒë·ªÉ tƒÉng t·ªëc
# 6. Gi·ªõi h·∫°n 10k items ƒë·∫ßu ti√™n
# 
# C√°ch s·ª≠ d·ª•ng:
# 1. S·ª≠a ph·∫ßn load DataFrame th·∫≠t trong main section
# 2. ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ·∫£nh trong `tiny_face_module` t·ªìn t·∫°i
# 3. Ch·∫°y script
# 
# Output:
# - Folder `json_results_df/`: Ch·ª©a file JSON cho t·ª´ng item_id
# - Format JSON t∆∞∆°ng th√≠ch v·ªõi h·ªá th·ªëng hi·ªán t·∫°i

import os
import json
import pandas as pd
import glob
import torch
import cv2
import numpy as np
from PIL import Image, ImageDraw
import re
import time
from multiprocessing import Pool, cpu_count

from models.experimental import attempt_load
from utils.general import non_max_suppression, scale_coords, check_img_size, set_logging
from utils.torch_utils import select_device, time_synchronized

# Config
model_path = "./yolov7-face.pt"
json_output_dir = "./all_json_results_df"
max_faces_images_dir = "./yolov7_all_max_faces_images"
base_image_path = "/mnt/md0/projects/auto_review_footage/"  # ƒê∆∞·ªùng d·∫´n g·ªëc ƒë·∫øn ·∫£nh
os.makedirs(json_output_dir, exist_ok=True)
os.makedirs(max_faces_images_dir, exist_ok=True)

NUM_GPU = 2  # S·ªë l∆∞·ª£ng GPU th·ª±c t·∫ø
MAX_ITEMS = 24000  # Gi·ªõi h·∫°n items
CONF_THRES = 0.6  # Confidence threshold
IOU_THRES = 0.3  # NMS threshold - Gi·∫£m t·ª´ 0.45 xu·ªëng 0.3 ƒë·ªÉ lo·∫°i b·ªè nhi·ªÅu box tr√πng l·∫∑p h∆°n
IMG_SIZE = 640    # Input image size

# C√°c gi√° tr·ªã IOU threshold khuy·∫øn ngh·ªã:
# 0.2-0.3: Lo·∫°i b·ªè nhi·ªÅu box tr√πng l·∫∑p (t·ªët cho face detection)
# 0.4-0.5: C√¢n b·∫±ng gi·ªØa lo·∫°i b·ªè tr√πng l·∫∑p v√† gi·ªØ l·∫°i detection
# 0.6-0.7: √çt lo·∫°i b·ªè box (c√≥ th·ªÉ c√≥ nhi·ªÅu box ch·ªìng l√™n nhau)

print(f"üîß NMS Config: CONF_THRES={CONF_THRES}, IOU_THRES={IOU_THRES}")

def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):
    """Resize v√† pad ·∫£nh ƒë·ªÉ ph√π h·ª£p v·ªõi stride"""
    shape = img.shape[:2]  # current shape [height, width]
    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape)

    # Scale ratio (new / old)
    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    if not scaleup:  # only scale down
        r = min(r, 1.0)

    # Compute padding
    ratio = r, r  # width, height ratios
    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding
    if auto:  # minimum rectangle
        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
    elif scaleFill:  # stretch
        dw, dh = 0.0, 0.0
        new_unpad = (new_shape[1], new_shape[0])
        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios

    dw /= 2  # divide padding into 2 sides
    dh /= 2

    if shape[::-1] != new_unpad:  # resize
        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
    return img, ratio, (dw, dh)

def normalize_bbox(bbox, img_width, img_height):
    """Chuy·ªÉn bbox t·ª´ pixel v·ªÅ normalized coordinates (0-1)"""
    if isinstance(bbox, (tuple, list)) and len(bbox) == 4:
        x1, y1, x2, y2 = bbox
    else:
        return None
    
    # Normalize v·ªÅ 0-1
    norm_x1 = x1 / img_width
    norm_y1 = y1 / img_height
    norm_x2 = x2 / img_width
    norm_y2 = y2 / img_height
    
    return [norm_x1, norm_y1, norm_x2, norm_y2]

def draw_faces_on_image(image, faces_data):
    """V·∫Ω bounding box l√™n ·∫£nh"""
    draw = ImageDraw.Draw(image)
    
    for face_data in faces_data:
        bbox = face_data['bbox']
        conf = face_data['conf']
        
        x1, y1, x2, y2 = bbox
        
        # V·∫Ω bounding box
        draw.rectangle([x1, y1, x2, y2], outline="red", width=3)
        
        # V·∫Ω confidence score
        conf_text = f"{conf:.2f}"
        draw.text((x1, y1-20), conf_text, fill="red")
    
    return image

def get_image_paths_from_base(base_path):
    """
    T·ª´ ƒë∆∞·ªùng d·∫´n base nh∆∞ 099/016/118/99016118_original.jpg
    T√¨m t·∫•t c·∫£ ·∫£nh _original_xxx.jpg trong c√πng th∆∞ m·ª•c
    """
    # N·ªëi v·ªõi ƒë∆∞·ªùng d·∫´n g·ªëc
    full_base_path = os.path.join(base_image_path, base_path)
    
    if not os.path.exists(full_base_path):
        return []
    
    # L·∫•y th∆∞ m·ª•c v√† t√™n file g·ªëc
    dir_path = os.path.dirname(full_base_path)
    base_name = os.path.basename(full_base_path)
    
    # T√°ch ph·∫ßn tr∆∞·ªõc _original.jpg
    if '_original.jpg' in base_name:
        prefix = base_name.replace('_original.jpg', '')
        # T√¨m t·∫•t c·∫£ file _original_xxx.jpg
        pattern = os.path.join(dir_path, f"{prefix}_original_*.jpg")
        return sorted(glob.glob(pattern))
    
    return []

def detect_faces_yolov7(image_path, model, device, img_size=640, conf_thres=None, iou_thres=None):
    """Detect faces b·∫±ng YOLOv7-Face model"""
    # S·ª≠ d·ª•ng config global n·∫øu kh√¥ng truy·ªÅn v√†o
    if conf_thres is None:
        conf_thres = CONF_THRES
    if iou_thres is None:
        iou_thres = IOU_THRES
        
    try:
        # Load image
        img0 = cv2.imread(image_path)  # BGR
        if img0 is None:
            return []
        
        # Letterbox
        img = letterbox(img0, img_size, stride=int(model.stride.max()))[0]
        
        # Convert
        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3xHxW
        img = np.ascontiguousarray(img)
        
        img = torch.from_numpy(img).to(device)
        img = img.half() if device.type != 'cpu' else img.float()  # uint8 to fp16/32
        img /= 255.0  # 0 - 255 to 0.0 - 1.0
        if img.ndimension() == 3:
            img = img.unsqueeze(0)
        
        # Inference
        with torch.no_grad():
            pred = model(img, augment=False)[0]
        
        # Debug: Ki·ªÉm tra shape c·ªßa prediction
        print(f"Prediction shape: {pred.shape}")
        
        # Apply NMS - s·ª≠a l·ªói tensor size mismatch
        try:
            # Th·ª≠ v·ªõi kpt_label=False tr∆∞·ªõc - s·ª≠ d·ª•ng thresholds ƒë∆∞·ª£c truy·ªÅn v√†o
            pred = non_max_suppression(pred, conf_thres, iou_thres, agnostic=False, kpt_label=False, nc=1)
        except RuntimeError as e:
            if "Expected size" in str(e) and "dimension" in str(e):
                print(f"NMS error: {e}")
                print(f"Original pred shape: {pred.shape}")
                # Fallback 1: Th·ª≠ v·ªõi kpt_label=True
                try:
                    pred = non_max_suppression(pred, conf_thres, iou_thres, agnostic=False, kpt_label=True, nc=1)
                except:
                    # Fallback 2: ch·ªâ l·∫•y ph·∫ßn bbox + conf (6 columns ƒë·∫ßu)
                    pred_fixed = pred[..., :6]  # x1, y1, x2, y2, conf, cls
                    print(f"Fixed pred shape: {pred_fixed.shape}")
                    pred = non_max_suppression(pred_fixed, conf_thres, iou_thres, agnostic=False, kpt_label=False, nc=1)
            else:
                raise e
        
        faces_data = []
        
        # Process detections
        for i, det in enumerate(pred):  # detections per image
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()
                
                # Extract face data
                for *xyxy, conf, cls in det:
                    x1, y1, x2, y2 = [float(x) for x in xyxy]
                    confidence = float(conf)
                    
                    # Normalize confidence n·∫øu c·∫ßn
                    if confidence > 1.0:
                        confidence = confidence / 100.0
                    confidence = max(0.0, min(1.0, confidence))
                    
                    faces_data.append({
                        'bbox': [x1, y1, x2, y2],
                        'conf': confidence
                    })
        
        return faces_data
        
    except Exception as e:
        print(f"L·ªói detect faces cho {image_path}: {e}")
        return []

def detect_item_images(item_data):
    """Detect faces cho t·∫•t c·∫£ ·∫£nh c·ªßa 1 item_id"""
    item_id, tiny_face_module = item_data
    
    try:
        # Kh·ªüi t·∫°o model cho m·ªói process
        device = select_device('')  # Auto select GPU/CPU
        model = attempt_load(model_path, map_location=device)
        
        if device.type != 'cpu':
            model.half()
        model.eval()
        
        # L·∫•y danh s√°ch ·∫£nh c·∫ßn predict
        image_paths = get_image_paths_from_base(tiny_face_module)
        
        if not image_paths:
            print(f"Kh√¥ng t√¨m th·∫•y ·∫£nh cho item_id {item_id} t·∫°i {tiny_face_module}")
            return None
        
        all_frames_data = []
        total_start_time = time.time()
        max_faces_count = 0
        max_faces_frame_data = None
        
        for frame_idx, img_path in enumerate(image_paths):
            try:
                start_time = time.time()
                
                # Detect faces
                faces_data = detect_faces_yolov7(img_path, model, device, IMG_SIZE)
                
                elapsed = time.time() - start_time
                
                # Load image ƒë·ªÉ l∆∞u sau n√†y
                image = Image.open(img_path).convert("RGB")
                img_width, img_height = image.size
                
                # Chu·∫©n b·ªã d·ªØ li·ªáu cho frame n√†y
                bboxes_data = []
                confidence_data = []
                class_names_data = []
                class_indexes_data = []
                class_groups_data = []
                
                # ƒêi·ªÅn d·ªØ li·ªáu cho c√°c face th·ª±c t·∫ø
                for face_data in faces_data:
                    norm_bbox = normalize_bbox(face_data['bbox'], img_width, img_height)
                    if norm_bbox:
                        bboxes_data.append(norm_bbox)
                        confidence_data.append(face_data['conf'])
                        class_names_data.append("face")
                        class_indexes_data.append(0)
                        class_groups_data.append("face")
                
                frame_data = {
                    "frame_idx": frame_idx,
                    "image_path": img_path,
                    "num_faces": len(faces_data),
                    "bboxes": bboxes_data,
                    "confidence": confidence_data,
                    "class_names": class_names_data,
                    "class_indexes": class_indexes_data,
                    "class_groups": class_groups_data,
                    "infer_time": elapsed,
                    "faces_data": faces_data,  # L∆∞u faces ƒë·ªÉ v·∫Ω sau n√†y
                    "image": image   # L∆∞u ·∫£nh ƒë·ªÉ v·∫Ω sau n√†y
                }
                
                all_frames_data.append(frame_data)
                
                # Theo d√µi frame c√≥ nhi·ªÅu face nh·∫•t
                if len(faces_data) > max_faces_count:
                    max_faces_count = len(faces_data)
                    max_faces_frame_data = frame_data
                
            except Exception as e:
                print(f"L·ªói x·ª≠ l√Ω frame {frame_idx} c·ªßa item {item_id}: {e}")
                continue
        
        total_elapsed = time.time() - total_start_time
        
        if not all_frames_data:
            print(f"Kh√¥ng c√≥ frame n√†o ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng cho item_id {item_id}")
            return None
        
        # T·∫°o JSON theo format y√™u c·∫ßu cho t·∫•t c·∫£ frames
        num_frames = len(all_frames_data)
        max_faces_per_frame = max([frame["num_faces"] for frame in all_frames_data]) if all_frames_data else 0
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu theo shape [num_frames, max_faces, 4]
        all_bboxes = []
        all_confidence = []
        all_class_names = []
        all_class_indexes = []
        all_class_groups = []
        
        for frame_data in all_frames_data:
            # Pad frame data to max_faces_per_frame
            frame_bboxes = frame_data["bboxes"] + [[-1.0, -1.0, -1.0, -1.0]] * (max_faces_per_frame - frame_data["num_faces"])
            frame_confidence = frame_data["confidence"] + [-1.0] * (max_faces_per_frame - frame_data["num_faces"])
            frame_class_names = frame_data["class_names"] + ["unknown"] * (max_faces_per_frame - frame_data["num_faces"])
            frame_class_indexes = frame_data["class_indexes"] + [-1] * (max_faces_per_frame - frame_data["num_faces"])
            frame_class_groups = frame_data["class_groups"] + ["unknown"] * (max_faces_per_frame - frame_data["num_faces"])
            
            all_bboxes.append(frame_bboxes)
            all_confidence.append(frame_confidence)
            all_class_names.append(frame_class_names)
            all_class_indexes.append(frame_class_indexes)
            all_class_groups.append(frame_class_groups)
        
        json_data = {
            "yolo_face_prediction": [
                {
                    "name": "yolo-face-bboxes",
                    "datatype": "FP32",
                    "shape": [num_frames, max_faces_per_frame, 4],
                    "data": all_bboxes
                },
                {
                    "name": "yolo-face-confidence",
                    "datatype": "FP32",
                    "shape": [num_frames, max_faces_per_frame],
                    "data": all_confidence
                },
                {
                    "name": "yolo-face-class_names",
                    "datatype": "BYTES",
                    "shape": [num_frames, max_faces_per_frame],
                    "data": all_class_names
                },
                {
                    "name": "yolo-face-class_indexes",
                    "datatype": "INT32",
                    "shape": [num_frames, max_faces_per_frame],
                    "data": all_class_indexes
                },
                {
                    "name": "yolo-face-class_groups",
                    "datatype": "BYTES",
                    "shape": [num_frames, max_faces_per_frame],
                    "data": all_class_groups
                },
                {
                    "name": "yolo-face-ckpt_version",
                    "datatype": "BYTES",
                    "shape": [num_frames],
                    "data": ["yolo_w6_face_v1"] * num_frames
                },
                {
                    "name": "yolo-face-infer_time",
                    "datatype": "FP32",
                    "shape": [num_frames],
                    "data": [frame_data["infer_time"] for frame_data in all_frames_data]
                },
                {
                    "name": "yolo-face-total_time",
                    "datatype": "FP32",
                    "shape": [1],
                    "data": [total_elapsed]
                }
            ]
        }
        
        # L∆∞u JSON
        json_path = os.path.join(json_output_dir, f"{item_id}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # L∆∞u ·∫£nh c√≥ nhi·ªÅu face nh·∫•t v·ªõi bounding box
        if max_faces_frame_data and max_faces_count > 0:
            try:
                # T·∫°o copy c·ªßa ·∫£nh ƒë·ªÉ v·∫Ω
                image_with_boxes = max_faces_frame_data["image"].copy()
                image_with_boxes = draw_faces_on_image(image_with_boxes, max_faces_frame_data["faces_data"])
                
                # L∆∞u ·∫£nh
                max_faces_image_path = os.path.join(max_faces_images_dir, f"{item_id}_max_{max_faces_count}_faces.jpg")
                image_with_boxes.save(max_faces_image_path, "JPEG", quality=95)
                print(f"Saved max faces image: {max_faces_image_path}")
            except Exception as e:
                print(f"L·ªói l∆∞u ·∫£nh max faces cho item {item_id}: {e}")
        
        total_faces = sum([frame["num_faces"] for frame in all_frames_data])
        print(f"Item {item_id}: {num_frames} frames, {total_faces} total faces, max {max_faces_count} faces/frame -> {json_path}")
        
        return (item_id, num_frames, total_faces, total_elapsed)
        
    except Exception as e:
        print(f"L·ªói x·ª≠ l√Ω item_id {item_id}: {e}")
        return None

# Main execution
if __name__ == "__main__":
    # TODO: Load DataFrame v·ªõi item_id v√† tiny_face_module columns
    # V√≠ d·ª•:
    # df = pd.read_csv("your_dataframe.csv")
    # df = pd.read_parquet("your_dataframe.parquet")
    
    # ƒê·ªÉ test, t·∫°o DataFrame m·∫´u
    # B·∫°n c·∫ßn thay th·∫ø ph·∫ßn n√†y b·∫±ng c√°ch load DataFrame th·ª±c t·∫ø
    print("WARNING: ƒêang s·ª≠ d·ª•ng DataFrame m·∫´u. H√£y thay th·∫ø b·∫±ng DataFrame th·ª±c t·∫ø c·ªßa b·∫°n!")
    df = pd.read_csv(
        "/home/dainguyenvan/.clearml/cache/storage_manager/datasets/ds_3d1c894fb7d54154a1d7bfc2d005bebc/val_data.csv"
    )
    
    # L·∫•y items ƒë·∫ßu ti√™n
    df_subset = df.head(MAX_ITEMS)
    print(f"S·∫Ω x·ª≠ l√Ω {len(df_subset)} items")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho multiprocessing
    items_data = [(row['item_id'], row['tiny_face_module']) for _, row in df_subset.iterrows()]
    
    # Gi·ªõi h·∫°n s·ªë process
    n_process = min(cpu_count(), NUM_GPU)
    if n_process < 1:
        n_process = 1
    
    print(f"S·ª≠ d·ª•ng {n_process} processes")
    print(f"Model: {model_path}")
    print(f"Confidence threshold: {CONF_THRES}")
    print(f"IoU threshold: {IOU_THRES}")
    print(f"Image size: {IMG_SIZE}")
    print(f"B·∫Øt ƒë·∫ßu predict cho {len(items_data)} items...")
    
    # Ch·∫°y multiprocessing
    with Pool(processes=n_process) as pool:
        results = pool.map(detect_item_images, items_data)
    
    # Th·ªëng k√™ k·∫øt qu·∫£
    successful_items = 0
    total_frames = 0
    total_faces = 0
    total_time = 0
    
    for result in results:
        if result is not None:
            item_id, num_frames, num_faces, elapsed = result
            successful_items += 1
            total_frames += num_frames
            total_faces += num_faces
            total_time += elapsed
    
    print(f"\n=== K·∫æT QU·∫¢ ===")
    print(f"Items x·ª≠ l√Ω th√†nh c√¥ng: {successful_items}/{len(items_data)}")
    print(f"T·ª∑ l·ªá th√†nh c√¥ng: {successful_items/len(items_data)*100:.1f}%")
    print(f"T·ªïng s·ªë frames: {total_frames}")
    print(f"T·ªïng s·ªë faces ph√°t hi·ªán: {total_faces}")
    print(f"Th·ªùi gian x·ª≠ l√Ω trung b√¨nh/item: {total_time/successful_items:.3f}s" if successful_items > 0 else "N/A")
    print(f"File JSON l∆∞u t·∫°i: {json_output_dir}")
    print(f"File ·∫£nh max faces l∆∞u t·∫°i: {max_faces_images_dir}")
    
    print(f"\nüéâ ƒê√É HO√ÄN TH√ÄNH: {successful_items} items th√†nh c√¥ng!")
    if successful_items < len(items_data):
        failed_items = len(items_data) - successful_items
        print(f"‚ö†Ô∏è  {failed_items} items th·∫•t b·∫°i")
    
    print(f"üìä T·ªïng c·ªông ƒë√£ x·ª≠ l√Ω: {successful_items}/{len(items_data)} items")
