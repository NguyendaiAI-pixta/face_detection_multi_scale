"""
Shared utilities and preprocessing functions for YOLOv7 Face Detection
Contains common functions that can be reused across multiple prediction scripts.
"""

import os
import json
import glob
import torch
import cv2
import numpy as np
from PIL import Image, ImageDraw
import time
from typing import List, Tuple, Dict, Any, Optional, Union

# Import t·ª´ YOLOv7 Face
from models.experimental import attempt_load
from utils.general import non_max_suppression, scale_coords
from utils.torch_utils import select_device

# Import for parent directory access
import sys
from pathlib import Path


def normalize_bbox(bbox, img_width, img_height):
    """
    Chuy·ªÉn bbox t·ª´ pixel v·ªÅ normalized coordinates (0-1)
    
    Args:
        bbox: tuple/list ch·ª©a (x1, y1, x2, y2) 
        img_width: chi·ªÅu r·ªông ·∫£nh
        img_height: chi·ªÅu cao ·∫£nh
        
    Returns:
        List normalized coordinates [x1, y1, x2, y2] trong kho·∫£ng [0, 1]
    """
    if isinstance(bbox, (tuple, list)) and len(bbox) == 4:
        x1, y1, x2, y2 = bbox
    else:
        return None
    
    # Normalize v·ªÅ 0-1
    norm_x1 = x1 / img_width
    norm_y1 = y1 / img_height
    norm_x2 = x2 / img_width
    norm_y2 = y2 / img_height
    
    return [norm_x1, norm_y1, norm_x2, norm_y2]


def denormalize_bbox(bbox, img_width, img_height):
    """
    Chuy·ªÉn bbox t·ª´ normalized coordinates (0-1) v·ªÅ pixel coordinates
    
    Args:
        bbox: tuple/list ch·ª©a normalized (x1, y1, x2, y2) 
        img_width: chi·ªÅu r·ªông ·∫£nh
        img_height: chi·ªÅu cao ·∫£nh
        
    Returns:
        List pixel coordinates [x1, y1, x2, y2]
    """
    if isinstance(bbox, (tuple, list)) and len(bbox) == 4:
        norm_x1, norm_y1, norm_x2, norm_y2 = bbox
    else:
        return None
    
    # Denormalize v·ªÅ pixel
    x1 = norm_x1 * img_width
    y1 = norm_y1 * img_height
    x2 = norm_x2 * img_width
    y2 = norm_y2 * img_height
    
    return [x1, y1, x2, y2]


def draw_faces_on_image(image, faces_data, bbox_color="red", text_color="red", line_width=3, 
                       use_api_preprocess=True, original_shape=None, img_size=None):
    """
    V·∫Ω bounding box l√™n ·∫£nh v·ªõi h·ªó tr·ª£ API preprocessing
    
    Args:
        image: PIL Image object
        faces_data: List c√°c dict ch·ª©a th√¥ng tin face {'bbox': [x1,y1,x2,y2], 'conf': float}
        bbox_color: m√†u c·ªßa bounding box
        text_color: m√†u c·ªßa text confidence
        line_width: ƒë·ªô d√†y c·ªßa ƒë∆∞·ªùng vi·ªÅn
        use_api_preprocess: c√≥ s·ª≠ d·ª•ng API preprocessing kh√¥ng
        original_shape: (H, W) shape c·ªßa ·∫£nh g·ªëc tr∆∞·ªõc khi preprocessing
        img_size: k√≠ch th∆∞·ªõc model input n·∫øu d√πng API preprocessing
        
    Returns:
        PIL Image v·ªõi bounding boxes ƒë∆∞·ª£c v·∫Ω
    """
    image_copy = image.copy()
    draw = ImageDraw.Draw(image_copy)
    
    for face_data in faces_data:
        bbox = face_data.get('bbox', [])
        conf = face_data.get('conf', 0.0)
        
        if len(bbox) >= 4:
            x1, y1, x2, y2 = bbox[:4]
            
            # ƒêi·ªÅu ch·ªânh t·ªça ƒë·ªô n·∫øu s·ª≠ d·ª•ng API preprocessing
            if use_api_preprocess and original_shape is not None and img_size is not None:
                x1, y1, x2, y2 = adjust_bbox_from_api_preprocess(
                    [x1, y1, x2, y2], original_shape, img_size, image.size
                )
            
            # V·∫Ω bounding box
            draw.rectangle([x1, y1, x2, y2], outline=bbox_color, width=line_width)
            
            # V·∫Ω confidence score
            conf_text = f"{conf:.2f}"
            draw.text((x1, max(0, y1-20)), conf_text, fill=text_color)
    
    return image_copy


def scale_coords_api_approach(img_input_shape, coords, img0_shape):
    """
    Scale coordinates cho API preprocessing approach (pad to square + letterbox)
    
    Args:
        img_input_shape: (H, W) - Shape c·ªßa model input (e.g. 640x640)
        coords: Tensor coordinates c·∫ßn scale
        img0_shape: (H, W, C) - Shape c·ªßa ·∫£nh g·ªëc
        
    Returns:
        Scaled coordinates v·ªÅ ·∫£nh g·ªëc
    """
    img_h, img_w = img_input_shape
    orig_h, orig_w = img0_shape[:2]
    
    # Debug info
    print(f"DEBUG scale_coords: input_shape={img_input_shape}, orig_shape={img0_shape[:2]}")
    print(f"DEBUG coords before scale: {coords[:2] if len(coords) > 0 else 'empty'}")
    
    # V·ªõi API approach: original ‚Üí pad to square ‚Üí letterbox to model input
    # Step 1: T√≠nh square size (max c·ªßa original dimensions)
    square_size = max(orig_h, orig_w)
    
    # Step 2: T√≠nh scale factor t·ª´ model input v·ªÅ square size
    scale_factor = square_size / img_h  # ƒê·∫£o ng∆∞·ª£c: t·ª´ small input v·ªÅ large square
    
    # Step 3: Scale coordinates t·ª´ model input v·ªÅ square size
    coords[:, [0, 2]] *= scale_factor  # x coordinates
    coords[:, [1, 3]] *= scale_factor  # y coordinates
    
    # Step 4: Clip v·ªÅ original image bounds (v√¨ pad top-left n√™n ch·ªâ c·∫ßn clip)
    coords[:, [0, 2]] = coords[:, [0, 2]].clamp(0, orig_w)  # clip x
    coords[:, [1, 3]] = coords[:, [1, 3]].clamp(0, orig_h)  # clip y
    
    print(f"DEBUG coords after scale: {coords[:2] if len(coords) > 0 else 'empty'}")
    return coords


def adjust_bbox_from_api_preprocess(bbox, original_shape, img_size, current_size):
    """
    Wrapper function ƒë·ªÉ gi·ªØ t∆∞∆°ng th√≠ch v·ªõi API c≈©
    S·ª≠ d·ª•ng scale_coords_api_approach b√™n trong
    
    Args:
        bbox: [x1, y1, x2, y2] - t·ªça ƒë·ªô bbox t·ª´ model
        original_shape: (H, W) - shape c·ªßa ·∫£nh g·ªëc
        img_size: k√≠ch th∆∞·ªõc input model (640)
        current_size: (W, H) - k√≠ch th∆∞·ªõc ·∫£nh hi·ªán t·∫°i ƒë·ªÉ v·∫Ω
        
    Returns:
        [x1, y1, x2, y2] - t·ªça ƒë·ªô ƒë√£ ƒëi·ªÅu ch·ªânh
    """
    import torch
    
    # Convert bbox to tensor format for scale_coords_api_approach
    coords_tensor = torch.tensor([[bbox[0], bbox[1], bbox[2], bbox[3]]], dtype=torch.float32)
    
    # T·∫°o img0_shape format (H, W, C) t·ª´ original_shape
    img0_shape = (original_shape[0], original_shape[1], 3)
    
    # Scale coordinates t·ª´ model input v·ªÅ ·∫£nh g·ªëc
    scaled_coords = scale_coords_api_approach((img_size, img_size), coords_tensor, img0_shape)
    
    # Extract scaled coordinates
    x1, y1, x2, y2 = scaled_coords[0].tolist()
    
    # Scale v·ªÅ k√≠ch th∆∞·ªõc ·∫£nh hi·ªán t·∫°i ƒë·ªÉ v·∫Ω
    orig_h, orig_w = original_shape
    curr_w, curr_h = current_size
    
    scale_w = curr_w / orig_w
    scale_h = curr_h / orig_h
    
    x1 = x1 * scale_w
    y1 = y1 * scale_h
    x2 = x2 * scale_w
    y2 = y2 * scale_h
    
    return [int(x1), int(y1), int(x2), int(y2)]


def get_image_paths_from_base(base_path, base_image_path="/mnt/md0/projects/auto_review_footage/"):
    """
    T·ª´ ƒë∆∞·ªùng d·∫´n base nh∆∞ 099/016/118/99016118_original.jpg
    T√¨m t·∫•t c·∫£ ·∫£nh _original_xxx.jpg trong c√πng th∆∞ m·ª•c
    
    Args:
        base_path: ƒë∆∞·ªùng d·∫´n relative t·ª´ base_image_path
        base_image_path: ƒë∆∞·ªùng d·∫´n g·ªëc ƒë·∫øn th∆∞ m·ª•c ch·ª©a ·∫£nh
        
    Returns:
        List c√°c ƒë∆∞·ªùng d·∫´n ·∫£nh t√¨m ƒë∆∞·ª£c
    """
    # N·ªëi v·ªõi ƒë∆∞·ªùng d·∫´n g·ªëc
    full_base_path = os.path.join(base_image_path, base_path)
    
    if not os.path.exists(full_base_path):
        return []
    
    # L·∫•y th∆∞ m·ª•c v√† t√™n file g·ªëc
    dir_path = os.path.dirname(full_base_path)
    base_name = os.path.basename(full_base_path)
    
    # T√°ch ph·∫ßn tr∆∞·ªõc _original.jpg
    if '_original.jpg' in base_name:
        prefix = base_name.replace('_original.jpg', '')
        # T√¨m t·∫•t c·∫£ file _original_xxx.jpg
        pattern = os.path.join(dir_path, f"{prefix}_original_*.jpg")
        image_paths = sorted(glob.glob(pattern))
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, th√™m file g·ªëc v√†o
        if not image_paths:
            image_paths = [full_base_path] if os.path.exists(full_base_path) else []
        
        return image_paths
    
    return [full_base_path] if os.path.exists(full_base_path) else []


def find_images_in_directory(dir_path, image_formats=None, recursive=False):
    """
    T√¨m t·∫•t c·∫£ file ·∫£nh trong th∆∞ m·ª•c
    
    Args:
        dir_path: ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c
        image_formats: list c√°c extension ·∫£nh c·∫ßn t√¨m (None = m·∫∑c ƒë·ªãnh)
        recursive: c√≥ t√¨m ki·∫øm recursive trong sub-folders kh√¥ng
        
    Returns:
        List ƒë∆∞·ªùng d·∫´n c√°c file ·∫£nh
    """
    if image_formats is None:
        image_formats = ['.jpg', '.jpeg', '.png', '.bmp', '.webp', '.tiff']
    
    image_paths = []
    
    if recursive:
        # T√¨m ki·∫øm recursive
        for root, dirs, files in os.walk(dir_path):
            for file in files:
                if any(file.lower().endswith(ext.lower()) for ext in image_formats):
                    image_paths.append(os.path.join(root, file))
    else:
        # T√¨m ki·∫øm trong th∆∞ m·ª•c hi·ªán t·∫°i
        for ext in image_formats:
            image_paths.extend(glob.glob(os.path.join(dir_path, f"*{ext}")))
            image_paths.extend(glob.glob(os.path.join(dir_path, f"*{ext.upper()}")))
    
    return sorted(image_paths)


def pad_to_square_top_left(img):
    """
    API Framework approach - pad to square (top-left alignment)
    
    Args:
        img: numpy array or PIL Image
        
    Returns:
        numpy array - ·∫£nh ƒë√£ pad th√†nh h√¨nh vu√¥ng
    """
    if isinstance(img, Image.Image):
        img = np.array(img)
    
    h, w, c = img.shape
    new_size = max(h, w)
    padded_img = np.zeros((new_size, new_size, c), dtype=img.dtype)
    padded_img[:h, :w, :] = img
    return padded_img


def letterbox_api(img, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True, stride=32):
    """
    Custom letterbox implementation for API approach
    Resize and pad image to fit new_shape while maintaining aspect ratio
    
    Args:
        img: input image (numpy array)
        new_shape: target shape (height, width)
        color: padding color (R, G, B)
        auto: minimum rectangle
        scaleFill: stretch
        scaleup: allow scale up
        stride: stride for padding alignment
        
    Returns:
        tuple: (processed_image, ratio, (dw, dh))
    """
    # Current shape [height, width]
    shape = img.shape[:2]  
    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape)

    # Scale ratio (new / old)
    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    if not scaleup:  # only scale down, do not scale up (for better val mAP)
        r = min(r, 1.0)

    # Compute padding
    ratio = r, r  # width, height ratios
    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding

    if auto:  # minimum rectangle
        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
    elif scaleFill:  # stretch
        dw, dh = 0.0, 0.0
        new_unpad = (new_shape[1], new_shape[0])
        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios

    dw /= 2  # divide padding into 2 sides
    dh /= 2

    if shape[::-1] != new_unpad:  # resize
        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
    
    return img, ratio, (dw, dh)


def preprocess_api_approach(img_path, img_size, stride):
    """
    API Framework preprocessing approach - EXACT MATCH v·ªõi arvutils.api
    Step 1: Pad image to square (top-left) 
    Step 2: Letterbox to model input size (s·ª≠ d·ª•ng YOLOv7 letterbox)
    Step 3: Convert to CHW format and make contiguous
    
    Args:
        img_path: ƒë∆∞·ªùng d·∫´n ·∫£nh
        img_size: k√≠ch th∆∞·ªõc target 
        stride: stride c·ªßa model
        
    Returns:
        numpy array - ·∫£nh ƒë√£ ƒë∆∞·ª£c preprocessing v·ªõi format CHW v√† contiguous
    """
    from utils.datasets import letterbox  # Import YOLOv7 letterbox function
    
    # Load image as PIL then convert to numpy (gi·ªëng API Framework)
    img_pil = Image.open(img_path).convert('RGB')
    img0 = np.array(img_pil)  # RGB format
    
    # Step 1: Pad to square (top-left) - EXACT match v·ªõi arvutils.api.pad_to_square_top_left
    squared_img = pad_to_square_top_left(img0)
    
    # Step 2: Letterbox using YOLOv7 function (EXACT match v·ªõi API Framework)
    img = letterbox(squared_img, img_size, stride=stride, auto=False)[0]
    
    # Step 3: Convert to CHW format and make contiguous 
    # EXACT match v·ªõi API Framework transforms:
    # img = img.transpose(2, 0, 1)  # HWC to CHW (NO BGR to RGB conversion!)
    img = img.transpose(2, 0, 1)  # HWC to CHW
    img = np.ascontiguousarray(img)
    
    return img


def load_yolo_model(model_path, device=''):
    """
    Load YOLOv7 model v·ªõi device ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    
    Args:
        model_path: ƒë∆∞·ªùng d·∫´n ƒë·∫øn file weights
        device: device ƒë·ªÉ load model ('' = auto, 'cpu', 'cuda:0', etc.)
        
    Returns:
        tuple: (model, device)
    """
    device = select_device(device)
    model = attempt_load(model_path, map_location=device)
    
    if device.type != 'cpu':
        model.half()
    model.eval()
    
    return model, device


def detect_faces_yolov7_basic(image_path, model, device, img_size=640, conf_thres=0.5, iou_thres=0.45):
    """
    Detect faces b·∫±ng YOLOv7-Face model (basic version)
    
    Args:
        image_path: ƒë∆∞·ªùng d·∫´n ·∫£nh
        model: YOLOv7 model ƒë√£ load
        device: torch device
        img_size: k√≠ch th∆∞·ªõc input
        conf_thres: confidence threshold
        iou_thres: IoU threshold cho NMS
        
    Returns:
        List c√°c face detection v·ªõi format [x1, y1, x2, y2, conf, cls]
    """
    try:
        # Preprocessing - ƒë√£ tr·∫£ v·ªÅ CHW format v√† contiguous
        img = preprocess_api_approach(image_path, img_size, int(model.stride.max()))
        
        # Load original image for scaling coordinates later
        img0 = cv2.imread(image_path)  # BGR
        if img0 is None:
            return []
        
        # Convert to tensor (img ƒë√£ l√† CHW format t·ª´ preprocess_api_approach)
        img = torch.from_numpy(img).to(device)
        img = img.half() if device.type != 'cpu' else img.float()  # uint8 to fp16/32
        img /= 255.0  # 0 - 255 to 0.0 - 1.0
        if img.ndimension() == 3:
            img = img.unsqueeze(0)
        
        # Inference
        with torch.no_grad():
            pred = model(img, augment=False)[0]
        
        # Apply NMS
        try:
            # Th·ª≠ v·ªõi kpt_label=False tr∆∞·ªõc
            pred = non_max_suppression(pred, conf_thres, iou_thres, agnostic=False, kpt_label=False, nc=1)
        except RuntimeError as e:
            if "Expected size" in str(e) and "dimension" in str(e):
                print(f"NMS error: {e}")
                # Fallback: Th·ª≠ v·ªõi kpt_label=True
                try:
                    pred = non_max_suppression(pred, conf_thres, iou_thres, agnostic=False, kpt_label=True, nc=1)
                except:
                    # Fallback 2: ch·ªâ l·∫•y ph·∫ßn bbox + conf (6 columns ƒë·∫ßu)
                    pred_fixed = pred[..., :6]  # x1, y1, x2, y2, conf, cls
                    pred = non_max_suppression(pred_fixed, conf_thres, iou_thres, agnostic=False, kpt_label=False, nc=1)
            else:
                raise e
        
        faces_data = []
        
        # Process detections
        for i, det in enumerate(pred):  # detections per image
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()
                
                # Extract face data
                for *xyxy, conf, cls in det:
                    x1, y1, x2, y2 = [int(x) for x in xyxy]
                    faces_data.append({
                        'bbox': [x1, y1, x2, y2],
                        'conf': float(conf),
                        'cls': int(cls) if cls is not None else 0
                    })
        
        return faces_data
        
    except Exception as e:
        print(f"L·ªói detect faces cho {image_path}: {e}")
        return []


def create_yolo_json_format(all_frames_data, item_id=None):
    """
    T·∫°o JSON theo format chu·∫©n cho YOLOv7 Face detection
    
    Args:
        all_frames_data: List c√°c dict ch·ª©a th√¥ng tin frame
        item_id: ID c·ªßa item (optional)
        
    Returns:
        Dict JSON data theo format y√™u c·∫ßu
    """
    if not all_frames_data:
        return None
    
    num_frames = len(all_frames_data)
    max_faces_per_frame = max([frame.get("num_faces", 0) for frame in all_frames_data])
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu theo shape [num_frames, max_faces, 4]
    all_bboxes = []
    all_confidence = []
    all_class_names = []
    all_class_indexes = []
    all_class_groups = []
    
    for frame_data in all_frames_data:
        # Pad frame data to max_faces_per_frame
        num_faces = frame_data.get("num_faces", 0)
        
        frame_bboxes = frame_data.get("bboxes", []) + [[-1.0, -1.0, -1.0, -1.0]] * (max_faces_per_frame - num_faces)
        frame_confidence = frame_data.get("confidence", []) + [-1.0] * (max_faces_per_frame - num_faces)
        frame_class_names = frame_data.get("class_names", []) + ["unknown"] * (max_faces_per_frame - num_faces)
        frame_class_indexes = frame_data.get("class_indexes", []) + [-1] * (max_faces_per_frame - num_faces)
        frame_class_groups = frame_data.get("class_groups", []) + ["unknown"] * (max_faces_per_frame - num_faces)
        
        all_bboxes.append(frame_bboxes)
        all_confidence.append(frame_confidence)
        all_class_names.append(frame_class_names)
        all_class_indexes.append(frame_class_indexes)
        all_class_groups.append(frame_class_groups)
    
    json_data = {
        "yolo_face_prediction": [
            {
                "name": "yolo-face-bboxes",
                "datatype": "FP32",
                "shape": [num_frames, max_faces_per_frame, 4],
                "data": all_bboxes
            },
            {
                "name": "yolo-face-confidence",
                "datatype": "FP32",
                "shape": [num_frames, max_faces_per_frame],
                "data": all_confidence
            },
            {
                "name": "yolo-face-class_names",
                "datatype": "BYTES",
                "shape": [num_frames, max_faces_per_frame],
                "data": all_class_names
            },
            {
                "name": "yolo-face-class_indexes",
                "datatype": "INT32",
                "shape": [num_frames, max_faces_per_frame],
                "data": all_class_indexes
            },
            {
                "name": "yolo-face-class_groups",
                "datatype": "BYTES",
                "shape": [num_frames, max_faces_per_frame],
                "data": all_class_groups
            },
            {
                "name": "yolo-face-ckpt_version",
                "datatype": "BYTES",
                "shape": [num_frames],
                "data": ["yolo_w6_face_v1"] * num_frames
            },
            {
                "name": "yolo-face-infer_time",
                "datatype": "FP32",
                "shape": [num_frames],
                "data": [frame_data.get("infer_time", 0.0) for frame_data in all_frames_data]
            },
            {
                "name": "yolo-face-total_time",
                "datatype": "FP32",
                "shape": [1],
                "data": [sum(frame_data.get("infer_time", 0.0) for frame_data in all_frames_data)]
            }
        ]
    }
    
    return json_data


def save_json_results(json_data, output_path, item_id=None):
    """
    L∆∞u k·∫øt qu·∫£ JSON
    
    Args:
        json_data: Dict ch·ª©a d·ªØ li·ªáu JSON
        output_path: ƒë∆∞·ªùng d·∫´n l∆∞u file ho·∫∑c th∆∞ m·ª•c
        item_id: ID c·ªßa item (d√πng ƒë·ªÉ t·∫°o t√™n file n·∫øu output_path l√† th∆∞ m·ª•c)
    """
    if json_data is None:
        return
    
    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n file cu·ªëi c√πng
    if os.path.isdir(output_path) or output_path.endswith('/'):
        # output_path l√† th∆∞ m·ª•c
        os.makedirs(output_path, exist_ok=True)
        filename = f"{item_id}.json" if item_id else "results.json"
        final_path = os.path.join(output_path, filename)
    else:
        # output_path l√† file
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        final_path = output_path
    
    # L∆∞u file JSON
    with open(final_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ Saved JSON results to: {final_path}")


def calculate_face_statistics(faces_data_list):
    """
    T√≠nh to√°n th·ªëng k√™ v·ªÅ face detection results
    
    Args:
        faces_data_list: List c√°c face detection results
        
    Returns:
        Dict ch·ª©a c√°c th·ªëng k√™
    """
    if not faces_data_list:
        return {"total_faces": 0, "avg_confidence": 0.0}
    
    total_faces = len(faces_data_list)
    confidences = [face.get('conf', 0.0) for face in faces_data_list]
    
    # T√≠nh to√°n bbox areas
    areas = []
    for face in faces_data_list:
        bbox = face.get('bbox', [])
        if len(bbox) >= 4:
            x1, y1, x2, y2 = bbox[:4]
            area = (x2 - x1) * (y2 - y1)
            areas.append(area)
    
    stats = {
        "total_faces": total_faces,
        "avg_confidence": np.mean(confidences) if confidences else 0.0,
        "min_confidence": min(confidences) if confidences else 0.0,
        "max_confidence": max(confidences) if confidences else 0.0,
        "avg_area": np.mean(areas) if areas else 0.0,
        "min_area": min(areas) if areas else 0.0,
        "max_area": max(areas) if areas else 0.0
    }
    
    return stats


def print_processing_summary(stats_list, total_time=None):
    """
    In ra t√≥m t·∫Øt qu√° tr√¨nh x·ª≠ l√Ω
    
    Args:
        stats_list: List c√°c dict th·ªëng k√™
        total_time: t·ªïng th·ªùi gian x·ª≠ l√Ω (optional)
    """
    if not stats_list:
        print("‚ùå No processing results to summarize")
        return
    
    total_items = len(stats_list)
    total_faces = sum(stats.get("total_faces", 0) for stats in stats_list)
    avg_confidence = np.mean([stats.get("avg_confidence", 0.0) for stats in stats_list])
    
    print(f"\n=== X·ª¨ L√ù HO√ÄN TH√ÄNH ===")
    print(f"üìä T·ªïng s·ªë items x·ª≠ l√Ω: {total_items}")
    print(f"üë• T·ªïng s·ªë faces ph√°t hi·ªán: {total_faces}")
    print(f"üìà Trung b√¨nh faces/item: {total_faces/total_items:.1f}")
    print(f"üéØ Confidence trung b√¨nh: {avg_confidence:.3f}")
    
    if total_time:
        print(f"‚è±Ô∏è  T·ªïng th·ªùi gian: {total_time:.2f}s")
        print(f"‚ö° Th·ªùi gian/item: {total_time/total_items:.3f}s")
